{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9279 samples, validate on 3093 samples\n",
      "Epoch 1/10\n",
      " 384/9279 [>.............................] - ETA: 13:09 - loss: 121.2050 - acc: 0.1406"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-1443eb2ec80e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m               metrics=['accuracy'])\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pickle_in = open(\"X.pickle\",\"rb\")\n",
    "X = pickle.load(pickle_in)\n",
    "\n",
    "\n",
    "pickle_in = open(\"y.pickle\",\"rb\")\n",
    "y = pickle.load(pickle_in)\n",
    "\n",
    "X = X/255.0\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "   tf.keras.layers.Conv2D(input_shape=(100,100,1),filters = 32,kernel_size=(2,2),strides = (1,1),padding='same',data_format='channels_last',activation = 'relu'),\n",
    "   tf.keras.layers.BatchNormalization(),\n",
    "   tf.keras.layers.MaxPool2D(),\n",
    "   tf.keras.layers.Dropout(0.25),\n",
    "   tf.keras.layers.Conv2D(filters = 64,kernel_size=(3,3),padding='same',data_format = 'channels_last',activation='relu'),\n",
    "   tf.keras.layers.BatchNormalization(),\n",
    "   tf.keras.layers.MaxPool2D(),\n",
    "   tf.keras.layers.Dropout(0.25),\n",
    "   tf.keras.layers.Conv2D(filters = 64,kernel_size=(3,3),strides = (1,1),padding='same',data_format='channels_last',activation = 'relu'),\n",
    "   tf.keras.layers.BatchNormalization(), \n",
    "   tf.keras.layers.MaxPool2D(),\n",
    "   tf.keras.layers.Dropout(0.25),\n",
    "   tf.keras.layers.Conv2D(filters = 128,kernel_size=(3,3),padding='same',data_format = 'channels_last',activation = 'relu'),\n",
    "   tf.keras.layers.BatchNormalization(),\n",
    "   tf.keras.layers.MaxPool2D(),\n",
    "   tf.keras.layers.Dropout(0.25),\n",
    "   tf.keras.layers.Conv2D(filters = 128,kernel_size=(3,3),padding='same',data_format = 'channels_last',activation = 'relu'),\n",
    "   tf.keras.layers.BatchNormalization(),\n",
    "   tf.keras.layers.MaxPool2D(),\n",
    "   tf.keras.layers.Dropout(0.25),\n",
    "   tf.keras.layers.Flatten(data_format = 'channels_last'),\n",
    "   tf.keras.layers.Dense(1024,activation = 'relu',kernel_regularizer = tf.keras.regularizers.l2(l = 0.09)),\n",
    "   tf.keras.layers.BatchNormalization(),\n",
    "   tf.keras.layers.Dense(256,activation = 'relu',kernel_regularizer = tf.keras.regularizers.l2(l = 0.09)),\n",
    "   tf.keras.layers.BatchNormalization(),\n",
    "   tf.keras.layers.Dense(10,activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, y, batch_size=32, epochs=10, validation_split=0.25)\n",
    "\n",
    "\n",
    "model.save('CNN.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(filepath):\n",
    "    IMG_SIZE = 100  # 50 in txt-based\n",
    "    img_array = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "    img_array=img_array/255.0  # filepathread in the image, convert to grayscale\n",
    "    new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize image to match model's expected sizing\n",
    "    return new_array.reshape(-1,IMG_SIZE, IMG_SIZE,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-prediction here is 6\n",
      "11-prediction here is 2\n",
      "43-prediction here is 2\n",
      "45-prediction here is 2\n",
      "47-prediction here is 2\n",
      "59-prediction here is 2\n",
      "61-prediction here is 2\n",
      "63-prediction here is 2\n",
      "65-prediction here is 2\n",
      "71-prediction here is 2\n",
      "79-prediction here is 2\n",
      "81-prediction here is 2\n",
      "85-prediction here is 2\n",
      "86-prediction here is 2\n",
      "87-prediction here is 2\n",
      "89-prediction here is 2\n",
      "90-prediction here is 2\n",
      "104-prediction here is 8\n",
      "107-prediction here is 0\n",
      "114-prediction here is 2\n",
      "121-prediction here is 2\n",
      "124-prediction here is 2\n",
      "126-prediction here is 2\n",
      "138-prediction here is 2\n",
      "139-prediction here is 2\n",
      "141-prediction here is 2\n",
      "143-prediction here is 2\n",
      "144-prediction here is 2\n",
      "173-prediction here is 2\n",
      "193-prediction here is 2\n",
      "194-prediction here is 2\n",
      "195-prediction here is 2\n",
      "197-prediction here is 2\n",
      "198-prediction here is 2\n",
      "217-prediction here is 2\n",
      "218-prediction here is 2\n",
      "219-prediction here is 2\n",
      "222-prediction here is 2\n",
      "240-prediction here is 2\n",
      "241-prediction here is 2\n",
      "243-prediction here is 2\n",
      "245-prediction here is 2\n",
      "247-prediction here is 2\n",
      "248-prediction here is 2\n",
      "249-prediction here is 2\n",
      "250-prediction here is 2\n",
      "251-prediction here is 2\n",
      "252-prediction here is 2\n",
      "253-prediction here is 2\n",
      "255-prediction here is 2\n",
      "295-prediction here is 2\n",
      "297-prediction here is 2\n",
      "301-prediction here is 2\n",
      "302-prediction here is 2\n",
      "303-prediction here is 2\n",
      "306-prediction here is 2\n",
      "307-prediction here is 2\n",
      "308-prediction here is 2\n",
      "309-prediction here is 2\n",
      "310-prediction here is 2\n",
      "311-prediction here is 2\n",
      "312-prediction here is 2\n",
      "343-prediction here is 2\n",
      "344-prediction here is 2\n",
      "345-prediction here is 2\n",
      "400-prediction here is 9\n",
      "430-prediction here is 6\n",
      "447-prediction here is 2\n",
      "449-prediction here is 2\n",
      "450-prediction here is 2\n",
      "457-prediction here is 2\n",
      "459-prediction here is 2\n",
      "460-prediction here is 2\n",
      "461-prediction here is 2\n",
      "462-prediction here is 2\n",
      "505-prediction here is 2\n",
      "506-prediction here is 2\n",
      "507-prediction here is 2\n",
      "515-prediction here is 2\n",
      "607-prediction here is 2\n",
      "608-prediction here is 2\n",
      "609-prediction here is 2\n",
      "610-prediction here is 2\n",
      "611-prediction here is 2\n",
      "612-prediction here is 2\n",
      "637-prediction here is 2\n",
      "639-prediction here is 2\n",
      "698-prediction here is 2\n",
      "701-prediction here is 2\n",
      "705-prediction here is 2\n",
      "727-prediction here is 7\n",
      "728-prediction here is 7\n",
      "729-prediction here is 7\n",
      "730-prediction here is 7\n",
      "731-prediction here is 7\n",
      "732-prediction here is 7\n",
      "751-prediction here is 2\n",
      "752-prediction here is 2\n",
      "753-prediction here is 2\n",
      "756-prediction here is 2\n",
      "766-prediction here is 9\n",
      "767-prediction here is 7\n",
      "768-prediction here is 9\n",
      "774-prediction here is 2\n",
      "821-prediction here is 2\n",
      "823-prediction here is 2\n",
      "828-prediction here is 2\n",
      "835-prediction here is 2\n",
      "836-prediction here is 2\n",
      "837-prediction here is 2\n",
      "839-prediction here is 2\n",
      "840-prediction here is 2\n",
      "856-prediction here is 7\n",
      "865-prediction here is 2\n",
      "867-prediction here is 2\n",
      "870-prediction here is 2\n",
      "941-prediction here is 2\n",
      "959-prediction here is 2\n",
      "991-prediction here is 2\n",
      "992-prediction here is 2\n",
      "993-prediction here is 2\n",
      "995-prediction here is 2\n",
      "996-prediction here is 2\n",
      "1002-prediction here is 2\n",
      "1008-prediction here is 2\n",
      "1074-prediction here is 2\n",
      "1081-prediction here is 2\n",
      "1083-prediction here is 2\n",
      "1123-prediction here is 2\n",
      "1124-prediction here is 2\n",
      "1125-prediction here is 2\n",
      "1126-prediction here is 2\n",
      "1127-prediction here is 2\n",
      "1128-prediction here is 2\n",
      "1134-prediction here is 9\n",
      "count is 135\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "model = tf.keras.models.load_model(\"CNN.model\")\n",
    "for i in range(1,1231):\n",
    "    prediction = model.predict([prepare(\"Newdataset/3/\" + str(i) + \".JPG\")])\n",
    "    prediction=np.argmax(prediction)\n",
    "    if(prediction!=3):\n",
    "        count = count + 1\n",
    "        print(str(i) + \"-prediction here is \" + str(prediction))\n",
    "print(\"count is \" + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
